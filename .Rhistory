incomplete <- 100 - sum(complete.cases(data))/nrow(data)*100
print(paste0(round(incomplete,2), "% of observations have at lest one missing feature"))
# Your code here
sum(is.na(data))
# Your code here
num <- sum(is.na(data))
tot_per <- num/(nrow(data)*ncol(data))
print(paste0(num," missing observations which is ", round(tot_per,2), "% of total"))
# Your code here
md.pairs(data)
# Your code here
t <- md.pattern(data)
View(t)
# Your code here
print("See md.pattern output plot above")
?titanic3
# Your code here
hist(data$age)
# Your code here
hist(data$age)
mean_imput <- data
mean_imput$x2[is.na(missing.data$x2)] = mean(mean_imput$x2, na.rm=TRUE)
# Your code here
hist(data$age)
mean_imput <- data
mean_imput$x2[is.na(mean_imput$x2)] = mean(mean_imput$x2, na.rm=TRUE)
# Your code here
hist(data$age)
mean_imput <- data
mean_imput$age[is.na(mean_imput$age)] = mean(mean_imput$age, na.rm=TRUE)
hist(mean_imput$age)
var(data$age)
var(data$age, na.rm = TRUE)
var(mean_imput$age, na.rm = TRUE)
sd(mean_imput$age, na.rm = TRUE)
sd(data$age, na.rm = TRUE)
install.packages("hmisc")
install.packages("Hmisc")
# Your code here
library(Hmisc)
imputed = impute(data$age, "random")
hist(imputed)
# Your code here
library(caret)
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
# Your code here
library(class)
?knn
kNN(data$fare, k=1)
remove.packages("VIM", lib="~/R/win-library/3.4")
install.packages("VIM")
# Your code here
library(VIM)
install.packages("car")
runif(5,1,2)
b0 <- 4
b1 <- .5
b2 <- 6
b3 <- 2
b4 <- 3
b5 <- 10
x1 <- runif(1000, 0,30)
x2 <- runif(1000, .5, 1.5)
x3 <- runif(1000, 7, 15)
x4 <- 2*x3 + rnorm(1000, 0, 5)
x4 <- .1*x3 + rnorm(1000, 0, 2)
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + rnorm(1000, 0, 3)
x4 <- 2*x3 + rnorm(1000, 0, 5)
x5 <- .1*x3 + rnorm(1000, 0, 2)
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + rnorm(1000, 0, 3)
print("correlation of x3 and x4:", cor(x3,x4))
print("correlation of x3 and x4:", cor(x3,x4))
print("correlation of x3 and x4:", cor(x3,x4)
cor(x3,x4)
cor(x3,x5)
cor(x3,x1)
df <- data.frame(y,x1,x2,x3,x4,x5)
View(df)
print("correlation of x3 and x4:", str(cor(x3,x4))
)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x3:",cor(x1,x3)))
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x3:",round(cor(x1,x3),2)))
print(paste("Correlation between x3 & x4:",round(cor(x3,x4),2)))
print(paste("Correlation between x3 & x5:",round(cor(x3,x5),2)))
lm_model <- lm(y ~ ., data = df)
summary(lm_model)
b6 <- 1
x6 <- .x1 + rnorm(1000, 0, .1)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
x6 <- x1 + rnorm(1000, 0, .1)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
x6 <- x1 + rnorm(1000, 0, .5)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
x6 <- x1 + rnorm(1000, 0, 1)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6 + rnorm(1000, 0, 3)
df <- data.frame(y,x1,x2,x3,x4,x5, x6)
lm_model <- lm(y ~ ., data = df)
summary(lm_model)
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6 + rnorm(1000, 0, 10)
df <- data.frame(y,x1,x2,x3,x4,x5, x6)
lm_model <- lm(y ~ ., data = df)
summary(lm_model)
b5 <- 3
x5 <- .1*x3 + rnorm(1000, 0, 2)
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6 + rnorm(1000, 0, 10)
df <- data.frame(y,x1,x2,x3,x4,x5, x6)
lm_model <- lm(y ~ ., data = df)
summary(lm_model)
x4 <- .5*x3 + rnorm(1000, 0, 5)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
print(paste("Correlation between x3 & x4:",round(cor(x3,x4),2)))
print(paste("Correlation between x3 & x5:",round(cor(x3,x5),2)))
x5 <- .1*x3 + rnorm(1000, 0, 1)
print(paste("Correlation between x3 & x5:",round(cor(x3,x5),2)))
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6 + rnorm(1000, 0, 10)
df <- data.frame(y,x1,x2,x3,x4,x5, x6)
lm_model <- lm(y ~ ., data = df)
summary(lm_model)
#convert to matrices for glm
x = model.matrix(y ~ ., df)
y = df$y
hist(y)
glm_model <- glmnet(x,y)
#libraries
library(glmnet)
glm_model <- glmnet(x,y)
summary(glm_model)
coef(glm_model)
dim(coef(glm_model))
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
plot(glm_model)
glm_model <- glmnet(x,y, alpha = .5)
plot(glm_model)
glm_model <- glmnet(x,y, alpha = .5, lambda = 1000)
plot(glm_model)
glm_model <- glmnet(x,y, alpha = .5, nlambda = 1000)
plot(glm_model)
glm_model <- glmnet(x,y, alpha = 0, nlambda = 1000)
plot(glm_model)
glm_model <- glmnet(x,y, alpha = 1, nlambda = 1000)
plot(glm_model)
library(plotmo)
install.packages("plotmo")
library(plotmo)
glm_plot(glm_model)
plot_glmnet(glm_model)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
x7 <- rnorm(1000, 0, 5) +7
df <- data.frame(y,x1,x2,x3,x4,x5, x6, x7)
lm_model <- lm(y ~ ., data = df)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
#convert to matrices for glm
x = model.matrix(y ~ ., df)
y = df$y
glm_model <- glmnet(x,y, alpha = 1, nlambda = 1000)
plot_glmnet(glm_model)
glm_model <- glmnet(x,y, alpha = 1, nlambda = 1000)
plot_glmnet(glm_model)
glm_model <- glmnet(x,y, alpha = .5, nlambda = 1000)
plot_glmnet(glm_model)
glm_model <- glmnet(x,y, alpha = 0, nlambda = 1000)
plot_glmnet(glm_model)
#number of observations
n = 100
b0 <- 4
b1 <- .5
b2 <- 6
b3 <- 2
b4 <- 3
b5 <- 3
b6 <- 1
x1 <- runif(n, 0,30)
x2 <- runif(n, .5, 1.5)
x3 <- runif(n, 7, 15)
x4 <- .5*x3 + rnorm(n, 0, 5)
x5 <- .1*x3 + rnorm(n, 0, 1)
x6 <- x1 + rnorm(n, 0, 1)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
print(paste("Correlation between x3 & x4:",round(cor(x3,x4),2)))
print(paste("Correlation between x3 & x5:",round(cor(x3,x5),2)))
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6 + rnorm(n, 0, 10)
x7 <- rnorm(n, 0, 5) +7
df <- data.frame(y,x1,x2,x3,x4,x5, x6, x7)
lm_model <- lm(y ~ ., data = df)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
#convert to matrices for glm
x = model.matrix(y ~ ., df)
y = df$y
glm_model <- glmnet(x,y, alpha = 0, nlambda = 100)
plot_glmnet(glm_model)
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)
View(x)
glm_fit <- cv.glmnet(x,y, alpha = 1)
plot(glm_fit)
glm_fit$lambda
glm_fit <- cv.glmnet(x,y, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best lambda value:",bestlambda))
glm_model <- glmnet(x,y, alpha = 1, lambda = bestlambda)
plot_glmnet(glm_model)
glm_model <- glmnet(x,y, alpha = 1, lambda = 100)
plot_glmnet(glm_model)
glm_model <- glmnet(x,y, alpha = 1, lambdas = 100)
glm_model <- glmnet(x,y, alpha = 1, lambdas = 100)
glm_model <- glmnet(x,y, alpha = 1, nlambdas = 100)
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)
print(paste("best lambda value:",bestlambda))
plot(glm_fit)
print(paste("best log lambda value:",bestlambda))
plot_glmnet(glm_model)
print(paste("best log lambda value:",round(bestlambda,2)))
plot_glmnet(glm_model)+ abline(x = bestlambda)
plot_glmnet(glm_model)+ abline(v = bestlambda)
plot(glm_fit)
plot_glmnet(glm_model)+ abline(v = bestlambda)
plot(glm_fit)
plot(sqrt(glm_fit))
?abline
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
print(paste("Correlation between x3 & x4:",round(cor(x3,x4),2)))
print(paste("Correlation between x3 & x5:",round(cor(x3,x5),2)))
x5 <- .5*x3 + rnorm(n, 0, 1)
x6 <- x1 + rnorm(n, 0, 1)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
print(paste("Correlation between x3 & x4:",round(cor(x3,x4),2)))
print(paste("Correlation between x3 & x5:",round(cor(x3,x5),2)))
y <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6 + rnorm(n, 0, 10)
x7 <- rnorm(n, 0, 5) +7
df <- data.frame(y,x1,x2,x3,x4,x5, x6, x7)
lm_model <- lm(y ~ ., data = df)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
#convert to matrices for glm
x = model.matrix(y ~ ., df)
y = df$y
glm_fit <- cv.glmnet(x,y, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
#checking for multicolinearity where we expected it:
print(paste("Correlation between x1 & x6:",round(cor(x1,x6),2)))
print(paste("Correlation between x3 & x4:",round(cor(x3,x4),2)))
print(paste("Correlation between x3 & x5:",round(cor(x3,x5),2)))
plot(glm_fit)
summary(glm_model)
summary(glm_fit)
hist(y)
rbinom(1,1,.3)
rbinom(1,1,.3)
rbinom(1,1,.3)
rbinom(1,1,.3)
rbinom(1,1,.3)
rbinom(1,1,.3)
rbinom(1,1,.3)
c1 <- sapply(y, rbinom(1,1,y/200))
#create categorical variable
cat_sample <- function(v){
return(rbinom(1,1,y/200))
}
c1 <- sapply(y, cat_sample)
df <- data.frame(y,x1,x2,x3,x4,x5, x6, x7,c1)
lm_model <- lm(y ~ ., data = df)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
#create categorical variable
cat_sample <- function(v){
return(rbinom(1,1,v/200))
}
c1 <- sapply(y, cat_sample)
x7 <- rnorm(n, 0, 5) +7
df <- data.frame(y,x1,x2,x3,x4,x5, x6, x7,c1)
lm_model <- lm(y ~ ., data = df)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
#convert to matrices for glm
x = model.matrix(y ~ ., df)
View(x)
y = df$y
glm_fit <- cv.glmnet(x,y, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
plot(glm_fit)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
df$c1 <- as.factor(df$c1)
lm_model <- lm(y ~ ., data = df)
#R2 of .8147 and RSE of 10.09 and we can see that x1 is not very significant
summary(lm_model)
#convert to matrices for glm
x = model.matrix(y ~ ., df)
y = df$y
glm_fit <- cv.glmnet(x,y, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
glm_fit <- cv.glmnet(x,y, alpha = 0)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
glm_model <- glmnet(x,y, alpha = 0, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
install.packages("glmnetUtils")
library(glmnetUtils)
glm_model <- glmnet(x,y, alpha = 0, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
glm_model <- glmnet(y~., data = df, alpha = 0, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
View(df)
glm_model$call
glm_model <- glmnet(x,y, data = df, alpha = 0, nlambda = 100)
glm_model <- glmnet(x,y, alpha = 0, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
glm_model <- glmnet(y~.,data = df, alpha = 0, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
cva.glmnet(y~.,data = df)
cross_alpha <- cva.glmnet(y~.,data = df)
plot(cross_alpha)
glm_fit <- cv.glmnet(x,y, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
glm_model <- glmnet(y~.,data = df, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
plot(cross_alpha)
plot(cross_alpha, title = "sfs")
plot(cross_alpha, main = "sfs")
plot(cross_alpha, main = "Lighter means alpha closer to 1 thus full lasso")
glm_fit <- cv.glmnet(y~.,data = df, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_fit <- cv.glmnet(y~.,data = df, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_model <- glmnet(y~.,data = df, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
glm_fit <- cv.glmnet(y~.,data = df, alpha = 1)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
plot(glm_fit)
glm_model <- glmnet(y~.,data = df, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
glm_fit <- cv.glmnet(y~.,data = df, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_fit <- cv.glmnet(y~.,data = df, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_fit <- cv.glmnet(y~.,data = df, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_fit <- cv.glmnet(y~.,data = df, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
View(x)
View(df)
setwd("~/Bootcamp/House_project")
library(tidyverse)
library(glmnet)
library(plotmo)
train <- read_csv("train.csv")
col_types <- sapply(train,class)
to_factor <- names(col_types[col_types == "character"])
#For character columns replace na with "NA"
#FOr numeric, repace na with 0
train_f <- train[to_factor]
train_n <- train%>%
select(-to_factor)
train_f[is.na(train_f)] <- "Data_Missing"
train_n[is.na(train_n)] <- 0
train_processed <- cbind(train_f, train_n)
train_processed[to_factor] <- lapply(train_processed[to_factor], as.factor)
View(train_processed)
x = model.matrix(SalePrice ~ ., train_processed)
y = train_processed$SalePrice
View(x)
size(x)
dim(x)
dim(x)[1]
print(paste("There are",dim(x)[2], "features w/ dummy variables"))
glm_fit <- cv.glmnet(x,y, alpha = 1)
plot(glm_fit)
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
bestlambda = glm_fit$lambda.min
print(paste("best log lambda value:",round(bestlambda,2)))
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
plot(glm_fit)
log(bestlambda)
bestlambda = log(glm_fit$lambda.min)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
class(glm_model)
plot(glm_fit)
glm_fit$nzero
glm_fit$glmnet.fit
print(paste("best log lambda value:",round(bestlambda,2)))
glm_fit$lambda
glm_fit$nzero
glm_fit$name
glm_fit$lambda.1se
13+09
1e+09
sqrt(1e+09)
summary(glm_model)
glm_model$glmnet.fit$dev.ratio[which(fitnet$glmnet.fit$lambda == fitnet$lambda.min)]
glm_fit$cvm/var(y)
min(glm_fit$cvm/var(y))
max(glm_fit$cvm/var(y))
1 - min(glm_fit$cvm/var(y))
print(paste("R Squared value of",R2))
#get R2
R2 <- 1 - min(glm_fit$cvm/var(y))
print(paste("R Squared value of",R2))
print(paste("R Squared value of",round(R2,2)))
knitr::opts_chunk$set(echo = TRUE)
Baseline_Model <- lm(SalePrice ~., data = train_processed)
summary(Baseline_Model)
print(paste("R Squared value of",round(R2,2)))
plot(glm_fit$cvm)
plot(glm_fit$cvm, glm_fit$lambda)
plot(glm_fit$lambda,glm_fit$cvm)
plot(log(glm_fit$lambda),glm_fit$cvm)
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
min(glm_fit$cvm)
sqrt(min(glm_fit$cvm))
CVK <- sqrt(min(glm_fit$cvm))
print(paste("R Squared value of best model",round(R2,2)))
print(paste("Cross validated MSE",round(CVK,2)))
var(y)
sd(y)
min(glm_fit$cvm)
glm_fit$lambda
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
glm_model <- glmnet(x,y, alpha = 1, nlambda = 100)
deviance(glm_model)
deviance(glm_fit)
y_hat <- predict(glm_model,X)
y_hat <- predict(glm_model,x)
View(y_hat)
bestlambda = glm_fit$lambda.min
y_hat <- predict(glm_model,x, s = bestlambda)
install.packages("mltools")
mse(y, y_hat)
library(MLtools)
library(mltools)
mse(y, y_hat)
cor(y, y_hat)
(cor(y, y_hat)^2
)
bestlambda
plot_glmnet(glm_model)+ abline(v = bestlambda, lwd = 2)
glm_fit$lambda
plot(glm_fit)
plot(glm_model)
plot(glm_fit)
mse(y, y_hat)
y_hat <- predict(glm_model,x, s = 500)
library(mltools)
mse(y, y_hat)
y_hat <- predict(glm_model,x, s = 100)
mse(y, y_hat)
glm_model <- glmnet(x,y, alpha = 1, lambda = bestlambda)
y_hat <- predict(glm_model,x, s = 100)
mse(y, y_hat)
cor(y, y_hat)
cor(y, y_hat)^2
glm_model$a0
glm_model$beta
glm_model$lambda
selected <- glm_model$beta
class(selected)
selected
View(selected)
names(selected)
selected[1]
selected[2]
selected[2,]
selected
glm_fit$nzero
